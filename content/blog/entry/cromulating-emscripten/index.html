---
title: >
 An Experiment in Squeezing Emscripted Code
slug: cromulating-emscripten
created: !!timestamp '2014-07-17 22:01:00.000000'
modified: !!timestamp '2014-07-17 22:01:00.000000'
tags: 
    - technology
    - mozilla
    - javascript

---

{% mark excerpt %}<p>Alternative title: <b>reduce your compressed file size with this one weird trick!</b></p>

<p>One of the top items on my list for <a href="https://www.pypyjs.org">PyPy.js</a> development is to reduce the size of the initial download &ndash; it currently weighs in at a hefty 3M of compressed javascript code plus another 2.4M of binary data.  The big wins here are clearly going to come from just generating less code, and I have made some promising initial progress on that front.  But it's also a good opportunity to experiment with some after-the-fact techniques for reducing the <i>compressed</i> filesize without having to change the actual generated code.</p>

<p>The obvious approach is to reach for a more performant compression algorithm, perhaps <a href="https://en.wikipedia.org/wiki/Bzip2">bzip2</a> or <a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm">LZMA</a>.  But these algorithms can suffer from slow decompression speeds and are not generally supported in today's web browsers.  For shipping content on the web today, gzip compression is the only game in town.</p>

<p>So can we do better while staying within the confines of gzip?</p>

{% endmark %}

<p>One example of such a technique is <a href="https://code.google.com/p/zopfli/">zopfli</a>.  This is a gzip-compatible compressor designed to squeeze every last byte out of a file, and the project page quotes an average of around 5% smaller output than a standard `gzip -9`.  The tradeoff: compression can take 100 times longer or more to complete, although decompression speed remains the same.  It is therefore highly unsuitable for on-demand compression applications, but extremely useful for delivery of web content that is compressed once and then transmitted many times.</p>

<p>Replacing my existing `gzip -9` compression with zopfli gives the following size reduction:</p>

<table style="border: 1px solid; width: 50%; white-space: nowrap;">
<tbody><tr><th style="width: 100%; padding: 0.5em">File</th><th>uncompressed</th><th>gzip -9</th><th>zopfli</th></tr>
<tr><td>pypy.js</td><td style="text-align: right">26M</td><td style="text-align: right">3.1M</td><td style="text-align: right">3.0M</td></tr>
<tr><td>pypy.js.mem</td><td style="text-align: right">9.1M</td><td style="text-align: right">2.4M</td><td style="text-align: right">2.3M</td></tr>
</tbody></table>

<p>That's not earth-shattering, but it's not bad for a one-time payment of some extra cpu cycles! Inspired by the zopfli philosophy, I decided to look for other ways to burn cpu in exchange for better gzip compression.</p>

<p>The <a href="https://en.wikipedia.org/wiki/LZ77_and_LZ78">techniques underlying gzip</a> are based on a sliding window approach.  The algorithm considers only a partial chunk of its input at any one time, and the particulars of the file format limit the size of this window to a maximum of 32K.  This means that if the input file contains similar chunks of code that happen to be spaced far apart, then gzip may not be able to take advantage of that similarity.</p>

<p>For arbitrary inputs there's nothing you can do about that, it's simply a limitation of the algorithm.  But I'm dealing with code, and chunks of code can be safely moved around.  So what would happen if we re-ordered the function definitions to put similar code closer together?</p>

<p>I threw together a fairly simple python script to implement this idea, and for want of an actual name for the technique, it's called <a href="https://github.com/rfk/pypyjs/blob/master/tools/cromulate.py">cromulate.py</a>.  It reads in an emscripten-generated javascript file, parses out each individual function definition, and measures the "similarity" between a pair of functions A and B by calculating `len(gzip(A)) + len(gzip(B)) - len(gzip(A + B))`.  It then uses a simple greedy selection algorithm to re-order the functions so that similar ones are closer together.  The result is a semantically-equivalent javascript file that will (hopefully) compress better than the original.</p>

<p>(If you squint, the underlying idea here is broadly similar to the way that bzip2 uses the <a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform">Burrows-Wheeler Transform</a>, which does no compression itself but which re-orders the data to make subsequent compression work better.  The trick of intuiting similarity by concatenating and compressing is not new either &ndash; see for example its application to linguistic analysis in <a href="http://arxiv.org/abs/cond-mat/0108530">Language Trees and Zipping</a>.)</p>

<p>Since a naive pairwise comparison of all functions would be O(n^2), I used a sliding window similar approach similar to gzip's to keep the running time under control.  The "--window-size" option limits the number of functions that will be considered at any one time.  By making the window larger than the 32KB limit of gzip, it is still able to find similarities that would otherwise go un-utilised.  Increasing the size of the window increases the running time, but can also improve the compressibility of the output.</p>

<p>To test it out, I used the existing emscripten-compiled programs from Alon's "<a href="https://github.com/kripken/Massive">Massive</a>" benchmark suite along with my own current pypy-js source file.  The results are summarized below:</p>

<img src="./resources/cromulation-results.png" />

<p>The first thing to note is that at small window sizes, the script can actually make the program <i>less</i> compressible.  But as the window size increases, and each function definition is compared to a larger portion of the file to find potential similarities, the technique is able to reduce the final compressed file size for all five programs in this test.</p>

<p>The "box2d" and "lua.vm" programs are fairly small, around a few hundred kilobytes, and the potential gain in compressibility is correspondingly small since similar functions are already likely to be close together.  "Poppler" and "sqlite" are larger at a few megabytes each, and show a modest but noticeable improvement of 1 to 2 percent from this approach.  And PyPy.js, at 26M and more than 16 thousand individual function definitions, sees an almost 10 percent improvement in final compressed size &ndash; from 26M to TODO.  That's pretty neat for such a naive optimization!</p>

<p>Of course, it takes several <i>hours</i> for the current version of my script to process PyPy.js with a large window size.  That's not the sort of thing you want to be running regularly.  But for a final deployment build the size saving is hard to ignore.</p>

<p>Will this ultimately be useful in practice?  I'm not sure.  It's possible that as I get around to reducing and optimizing the code generated for PyPy.js, the relative benefit of this approach will grow smaller and smaller.  But based on the results from other programs seen above, it seems unlikely to go to zero.</p>

